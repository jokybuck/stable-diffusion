{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1838icaNqVWNXQvAeooNpWgD0lrYVAB-T",
      "authorship_tag": "ABX9TyOIbnzsYG+85qnt8Hf0ocpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokybuck/stable-diffusion/blob/main/stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJDCqFpDxdxs",
        "outputId": "65ca041b-d6af-4be7-84f0-7b1a50cc1642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 10 14:44:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.3\n",
        "!pip install diffusers\n",
        "!pip install opencv-python==4.1.2.30\n",
        "!pip install pudb==2019.2\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.2\n",
        "!pip install pytorch-lightning\n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install test-tube>=0.7.5\n",
        "!pip install streamlit>=0.73.1\n",
        "!pip install einops==0.3.0\n",
        "!pip install torch-fidelity==0.3.0\n",
        "!pip install transformers==4.19.2\n",
        "!pip install kornia\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install invisible-watermark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3fLpT13fJLKm",
        "outputId": "5f441016-efbf-4cc3-a024-cd83a1a31d4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.3\n",
            "  Downloading albumentations-0.4.3.tar.gz (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3) (1.7.3)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.4/631.4 KB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.3) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2022.10.10)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.8.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (3.0.9)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60778 sha256=0218bdd156ed950f7ae7e5d3ca2a33249c23f71ba0c679165d086d28b9d33290\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/37/4e/0bd417ba6a58f73329b825623d8c949e8e4ac2cdbd252b786d\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654019 sha256=2ff78e08e8560d4695d86ea52955eac4f893d989312018f6e6ca4cfcc64326d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/23/e8/b1016c275f713978d312621da3c4f55920ec4297798aba8a5a\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.4.0\n",
            "    Uninstalling imgaug-0.4.0:\n",
            "      Successfully uninstalled imgaug-0.4.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-0.4.3 imgaug-0.2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.11.1-py3-none-any.whl (524 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.9/524.9 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers) (5.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from diffusers) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from diffusers) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from diffusers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from diffusers) (2022.6.2)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.0->diffusers) (6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->diffusers) (4.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.10.0->diffusers) (3.0.9)\n",
            "Installing collected packages: huggingface-hub, diffusers\n",
            "Successfully installed diffusers-0.11.1 huggingface-hub-0.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python==4.1.2.30\n",
            "  Downloading opencv_python-4.1.2.30-cp38-cp38-manylinux1_x86_64.whl (28.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/28.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python==4.1.2.30) (1.21.6)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "Successfully installed opencv-python-4.1.2.30\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pudb==2019.2\n",
            "  Downloading pudb-2019.2.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urwid>=1.1.1\n",
            "  Downloading urwid-2.1.2.tar.gz (634 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.6/634.6 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygments>=1.0 in /usr/local/lib/python3.8/dist-packages (from pudb==2019.2) (2.6.1)\n",
            "Building wheels for collected packages: pudb, urwid\n",
            "  Building wheel for pudb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pudb: filename=pudb-2019.2-py3-none-any.whl size=63247 sha256=0e0c85997f6c19c2b62950ac24c2f0b1e1895e49fa801bf3a759e4e10095557f\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/83/f1/d8a09d401e2512bfda01ac9fc1b334885f9ddf51617c1c49f1\n",
            "  Building wheel for urwid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for urwid: filename=urwid-2.1.2-cp38-cp38-linux_x86_64.whl size=257379 sha256=919dae3242e85868d4bc197411f78054040cc04572e16e69e035d9e9e59d6f62\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/71/e4/38b5d81438105d0e3db5016cf2eea6fa796d89d96a04451d4d\n",
            "Successfully built pudb urwid\n",
            "Installing collected packages: urwid, pudb\n",
            "Successfully installed pudb-2019.2 urwid-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.9.0) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio==2.9.0) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio-ffmpeg==0.4.2\n",
            "  Downloading imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Installing collected packages: tensorboardX, torchmetrics, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.5.0 pytorch-lightning-1.8.6 tensorboardX-2.5.1 torchmetrics-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omegaconf==2.1.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf==2.1.1) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=cfff4bf928d9712c6c0508ab2f787752f9d91a3dcbc373e3304214029e9186d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.8 omegaconf-2.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops==0.3.0\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-fidelity==0.3.0\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (1.7.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (0.14.0+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-fidelity==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torch-fidelity==0.3.0) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->torch-fidelity==0.3.0) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torch-fidelity==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torch-fidelity==0.3.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torch-fidelity==0.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->torch-fidelity==0.3.0) (4.0.0)\n",
            "Installing collected packages: torch-fidelity\n",
            "Successfully installed torch-fidelity-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.19.2\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (0.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.19.2) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.19.2) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.19.2) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.19.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.19.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.19.2) (1.24.3)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.6.9-py2.py3-none-any.whl (569 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from kornia) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.1->kornia) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->kornia) (3.0.9)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
            "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/taming-transformers.git /content/src/taming-transformers\n",
            "  Resolved https://github.com/CompVis/taming-transformers.git to commit 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->taming-transformers) (4.4.0)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
            "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /content/src/clip\n",
            "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip) (0.14.0+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2.10)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting invisible-watermark\n",
            "  Downloading invisible_watermark-0.1.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from invisible-watermark) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from invisible-watermark) (1.13.0+cu116)\n",
            "Requirement already satisfied: Pillow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from invisible-watermark) (7.1.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from invisible-watermark) (4.1.2.30)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from invisible-watermark) (1.4.1)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->invisible-watermark) (4.4.0)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime->invisible-watermark) (1.7.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime->invisible-watermark) (21.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime->invisible-watermark) (1.12)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->onnxruntime->invisible-watermark) (3.0.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime->invisible-watermark) (1.2.1)\n",
            "Installing collected packages: protobuf, humanfriendly, onnx, coloredlogs, onnxruntime, invisible-watermark\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 invisible-watermark-0.1.5 onnx-1.13.0 onnxruntime-1.13.1 protobuf-3.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBXFL5IFJ2R8",
        "outputId": "862b7c3c-97d1-4cfb-da07-f9b6141ad14a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.9.0 requires jedi, which is not installed.\n",
            "tensorflow 2.9.2 has requirement protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3.\n",
            "tensorboardx 2.5.1 has requirement protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3.\n",
            "tensorboard 2.9.1 has requirement protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CompVis/stable-diffusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGP8rGEYKGM6",
        "outputId": "2e9cf067-20e3-4172-9907-1ebd35c9fd52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stable-diffusion'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 42.65 MiB | 19.16 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stable-diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEahyTKKNJB",
        "outputId": "1444618f-38de-4ef1-fc52-f8640e385d63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stable-diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vjLH0RnKTfa",
        "outputId": "4bd1f52d-67ce-470b-f5e3-58870965fdf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/stable-diffusion\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->latent-diffusion==0.0.1) (4.4.0)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/txt2img.py --prompt \"Mt. Fuji in the style of Gauguin\" \\\n",
        "                           --plms \\\n",
        "                           --ckpt /content/drive/MyDrive/Models/sd-v1-4.ckpt \\\n",
        "                           --skip_grid \\\n",
        "                           --n_samples 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v_-cLtTKgt0",
        "outputId": "e04e789e-b5b3-4b11-ec46-0390a811d959"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 342/342 [00:00<00:00, 400kB/s]\n",
            "Downloading: 100% 4.44k/4.44k [00:00<00:00, 4.50MB/s]\n",
            "Downloading: 100% 1.13G/1.13G [00:17<00:00, 68.4MB/s]\n",
            "Global seed set to 42\n",
            "Loading model from /content/drive/MyDrive/Models/sd-v1-4.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Downloading: 100% 939k/939k [00:00<00:00, 2.58MB/s]\n",
            "Downloading: 100% 512k/512k [00:00<00:00, 2.11MB/s]\n",
            "Downloading: 100% 389/389 [00:00<00:00, 307kB/s]\n",
            "Downloading: 100% 905/905 [00:00<00:00, 726kB/s]\n",
            "Downloading: 100% 4.41k/4.41k [00:00<00:00, 3.49MB/s]\n",
            "Downloading: 100% 1.59G/1.59G [00:25<00:00, 67.6MB/s]\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/2 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:04<03:39,  4.47s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:04<01:35,  1.99s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:04<00:56,  1.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:05<00:38,  1.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:05<00:28,  1.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:05<00:22,  1.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:06<00:18,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:06<00:15,  2.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:06<00:13,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:06<00:12,  3.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:07<00:11,  3.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:07<00:10,  3.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:07<00:10,  3.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:07<00:09,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:08<00:09,  3.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:08<00:09,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:08<00:08,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:08<00:08,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:09<00:08,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:09<00:07,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:09<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:09<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:10<00:07,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:10<00:06,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:10<00:06,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:11<00:06,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:11<00:06,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:11<00:05,  3.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:11<00:05,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:12<00:05,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:12<00:05,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:12<00:04,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:12<00:04,  3.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:13<00:04,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:13<00:03,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:13<00:03,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:13<00:03,  3.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:14<00:03,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:14<00:02,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:14<00:02,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:15<00:02,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:15<00:02,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:15<00:01,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:15<00:01,  3.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:16<00:01,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:16<00:01,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:16<00:00,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:16<00:00,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:17<00:00,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:17<00:00,  2.87it/s]\n",
            "\n",
            "data: 100% 1/1 [00:24<00:00, 24.66s/it]\n",
            "Sampling:  50% 1/2 [00:24<00:24, 24.66s/it]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:00<00:28,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:00<00:19,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:01<00:15,  2.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:01<00:14,  3.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:01<00:13,  3.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:01<00:12,  3.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:02<00:11,  3.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:02<00:11,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:02<00:11,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:02<00:10,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:03<00:10,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:03<00:10,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:03<00:09,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:04<00:09,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:04<00:09,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:04<00:09,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:04<00:08,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:05<00:08,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:05<00:08,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:05<00:08,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:05<00:07,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:06<00:07,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:06<00:07,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:06<00:06,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:07<00:06,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:07<00:06,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:07<00:06,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:07<00:05,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:08<00:05,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:08<00:05,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:08<00:05,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:08<00:04,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:09<00:04,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:09<00:04,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:09<00:04,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:09<00:03,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:10<00:03,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:10<00:03,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:10<00:02,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:11<00:02,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:11<00:02,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:11<00:02,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:11<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:12<00:01,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:12<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:12<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:12<00:00,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:13<00:00,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:13<00:00,  3.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:13<00:00,  3.63it/s]\n",
            "\n",
            "data: 100% 1/1 [00:17<00:00, 17.14s/it]\n",
            "Sampling: 100% 2/2 [00:41<00:00, 20.90s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/txt2img.py --prompt \"Mt. Fuji in the style of Pablo Picasso\" \\\n",
        "                           --plms \\\n",
        "                           --ckpt /content/drive/MyDrive/Models/sd-v1-4.ckpt \\\n",
        "                           --skip_grid \\\n",
        "                           --n_samples 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4qXGzmbM2Fk",
        "outputId": "a496698e-d835-4536-e508-1b15dc4d7bd3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 42\n",
            "Loading model from /content/drive/MyDrive/Models/sd-v1-4.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
            "Sampling:   0% 0/2 [00:00<?, ?it/s]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:04<03:34,  4.38s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:04<01:34,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:04<00:55,  1.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:05<00:37,  1.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:05<00:27,  1.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:05<00:21,  2.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:05<00:18,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:06<00:15,  2.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:06<00:13,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:06<00:12,  3.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:07<00:11,  3.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:07<00:11,  3.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:07<00:10,  3.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:07<00:10,  3.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:08<00:09,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:08<00:09,  3.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:08<00:08,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:08<00:08,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:09<00:08,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:09<00:08,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:09<00:07,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:09<00:07,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:10<00:07,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:10<00:06,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:10<00:06,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:11<00:06,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:11<00:06,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:11<00:05,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:11<00:05,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:12<00:05,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:12<00:05,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:12<00:04,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:12<00:04,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:13<00:04,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:13<00:04,  3.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:13<00:03,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:13<00:03,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:14<00:03,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:14<00:02,  3.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:14<00:02,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:15<00:02,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:15<00:02,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:15<00:01,  3.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:15<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:16<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:16<00:01,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:16<00:00,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:16<00:00,  3.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:17<00:00,  3.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:17<00:00,  2.85it/s]\n",
            "\n",
            "data: 100% 1/1 [00:24<00:00, 24.58s/it]\n",
            "Sampling:  50% 1/2 [00:24<00:24, 24.58s/it]\n",
            "data:   0% 0/1 [00:00<?, ?it/s]\u001b[AData shape for PLMS sampling is (1, 4, 64, 64)\n",
            "Running PLMS Sampling with 50 timesteps\n",
            "\n",
            "\n",
            "PLMS Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   2% 1/50 [00:00<00:29,  1.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   4% 2/50 [00:00<00:19,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   6% 3/50 [00:01<00:16,  2.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:   8% 4/50 [00:01<00:14,  3.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  10% 5/50 [00:01<00:13,  3.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  12% 6/50 [00:01<00:12,  3.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  14% 7/50 [00:02<00:12,  3.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  16% 8/50 [00:02<00:11,  3.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  18% 9/50 [00:02<00:11,  3.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  20% 10/50 [00:03<00:11,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  22% 11/50 [00:03<00:10,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  24% 12/50 [00:03<00:10,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  26% 13/50 [00:03<00:10,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  28% 14/50 [00:04<00:09,  3.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  30% 15/50 [00:04<00:09,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  32% 16/50 [00:04<00:09,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  34% 17/50 [00:04<00:09,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  36% 18/50 [00:05<00:08,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  38% 19/50 [00:05<00:08,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  40% 20/50 [00:05<00:08,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  42% 21/50 [00:06<00:07,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  44% 22/50 [00:06<00:07,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  46% 23/50 [00:06<00:07,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  48% 24/50 [00:06<00:07,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  50% 25/50 [00:07<00:06,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  52% 26/50 [00:07<00:06,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  54% 27/50 [00:07<00:06,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  56% 28/50 [00:07<00:06,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  58% 29/50 [00:08<00:05,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  60% 30/50 [00:08<00:05,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  62% 31/50 [00:08<00:05,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  64% 32/50 [00:09<00:04,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  66% 33/50 [00:09<00:04,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  68% 34/50 [00:09<00:04,  3.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  70% 35/50 [00:09<00:04,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  72% 36/50 [00:10<00:03,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  74% 37/50 [00:10<00:03,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  76% 38/50 [00:10<00:03,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  78% 39/50 [00:10<00:03,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  80% 40/50 [00:11<00:02,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  82% 41/50 [00:11<00:02,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  84% 42/50 [00:11<00:02,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  86% 43/50 [00:12<00:01,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  88% 44/50 [00:12<00:01,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  90% 45/50 [00:12<00:01,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  92% 46/50 [00:12<00:01,  3.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  94% 47/50 [00:13<00:00,  3.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  96% 48/50 [00:13<00:00,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler:  98% 49/50 [00:13<00:00,  3.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "PLMS Sampler: 100% 50/50 [00:14<00:00,  3.57it/s]\n",
            "\n",
            "data: 100% 1/1 [00:17<00:00, 17.47s/it]\n",
            "Sampling: 100% 2/2 [00:42<00:00, 21.02s/it]\n",
            "Your samples are ready and waiting for you here: \n",
            "outputs/txt2img-samples \n",
            " \n",
            "Enjoy.\n"
          ]
        }
      ]
    }
  ]
}